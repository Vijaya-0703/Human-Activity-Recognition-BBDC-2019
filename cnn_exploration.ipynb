{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/markkoerner/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.utils import normalize\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Prateeks NN notebook\n",
    "\n",
    "# set the variables for the parent data folder and train file\n",
    "dataFolder = '/Users/markkoerner/PythonProjects/BBDCActivityClassification/bbdc_2019_Bewegungsdaten/original_dataset/'\n",
    "mainFolder = '/Users/markkoerner/PythonProjects/BBDCActivityClassification/'\n",
    "trainData = 'train.csv'\n",
    "\n",
    "# make a list of all the csv files in the parent and sub directories\n",
    "result = list(Path(dataFolder).rglob(\"Subject*.[cC][sS][vV]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8139"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code does not work properly. The code after #windowing is supposed to create windows of 100 observations, return the mean of each window and then create a new array with all the means. As of now, a large portion of the new dataframe contains NaNs, so using the resulting feature array as input for a CNN will result in the model failing to converge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptation of Prateeks code for generating dataset\n",
    "\n",
    "# Number of files to load\n",
    "numFiles = 6385\n",
    "\n",
    "# Maximum file size\n",
    "maxlen = 8000\n",
    "\n",
    "# NP Arrays to store the data and the label\n",
    "labelArr = np.empty((numFiles, 1))\n",
    "npArr = np.empty((numFiles, 160, 19))\n",
    "indxArr = []\n",
    "\n",
    "# read the training data into a dataframe\n",
    "trainDf = pd.read_csv(dataFolder+trainData)\n",
    "trainDf = trainDf[trainDf.Label != 'lay']\n",
    "# factorize the labels\n",
    "factor = pd.factorize(trainDf.Label)\n",
    "trainDf.Label = factor[0]\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for eachFile in result:\n",
    "  # Only read the dataset with labels available\n",
    "    if sum(str(eachFile)[-35:] == trainDf.Datafile)>0:\n",
    "        indxArr.append(str(eachFile)[-35:])\n",
    "        file = np.loadtxt(str(eachFile), delimiter=',')\n",
    "        #min max scale the inpuit\n",
    "        file = (file - file.mean(axis=1, keepdims=True))\n",
    "        \n",
    "        #zero pad to conserve last values\n",
    "        #padFile = np.zeros((50, 19))\n",
    "        #file = np.concatenate((file, padFile))\n",
    "        \n",
    "        #windowing\n",
    "        df = pd.DataFrame(file)\n",
    "        emg = df.iloc[:, :4]\n",
    "        other = df.iloc[::10, 4:]\n",
    "        emg = emg ** 2\n",
    "        emg = emg.rolling(100).mean().dropna(axis = 0, how = 'any').reset_index(drop = True)\n",
    "        emg = emg.iloc[::50, :].reset_index(drop = True)\n",
    "        emg = emg ** 2\n",
    "        other = other.rolling(10).mean().dropna(axis = 0, how ='any').iloc[::5, :].reset_index(drop = True)\n",
    "        df = pd.concat([emg, other], axis = 'columns')\n",
    "        \n",
    "        file = df.values\n",
    "        #extend to max length\n",
    "        file = (file - file.min(axis=1, keepdims=True)) / (file.max(axis = 1, keepdims = True) - file.min(axis = 1, keepdims = True))\n",
    "        padFile = np.zeros((159, 19))\n",
    "        file = np.concatenate((file, padFile))\n",
    "        file = file[:160]\n",
    "        \n",
    "        data = np.expand_dims(file, axis=0)\n",
    "        npArr[cnt] = data\n",
    "        labelArr[cnt] = trainDf[trainDf.Datafile == str(eachFile)[-35:]].Label\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create test array\n",
    "\n",
    "# Maximum file size\n",
    "maxlen = 8000\n",
    "\n",
    "# read the training data into a dataframe\n",
    "testDf = pd.read_csv(dataFolder+'challenge.csv')\n",
    "\n",
    "#number of files to read\n",
    "numFiles = testDf.shape[0]\n",
    "\n",
    "testArr = np.empty((numFiles, 160, 19))\n",
    "indyArr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same process for testing set\n",
    "cnt = 0\n",
    "for eachFile in result:\n",
    "  # Only read the dataset with labels available\n",
    "    if sum(str(eachFile)[-35:] == testDf.Datafile)>0:\n",
    "        indyArr.append(str(eachFile)[-35:])\n",
    "        file = np.loadtxt(str(eachFile), delimiter=',')\n",
    "        #min max scale the inpuit\n",
    "        file = (file - file.mean(axis=1, keepdims=True))\n",
    "        \n",
    "        #zero pad to conserve last values\n",
    "        padFile = np.zeros((50, 19))\n",
    "        file = np.concatenate((file, padFile))\n",
    "        \n",
    "        #windowing\n",
    "        df = pd.DataFrame(file)\n",
    "        emg = df.iloc[:, :4]\n",
    "        other = df.iloc[::10, 4:]\n",
    "        emg = emg ** 2\n",
    "        emg = emg.rolling(100).mean().dropna(axis = 'index').iloc[::50, :].reset_index(drop = True)\n",
    "        other = other.rolling(10).mean().dropna(axis = 'index').iloc[::5, :].reset_index(drop = True)\n",
    "        df = pd.concat([emg, other], axis = 1)\n",
    "        \n",
    "        file = df.values\n",
    "        #extend to max length\n",
    "        file = (file - file.min(axis=1, keepdims=True)) / (file.max(axis = 1, keepdims = True) - file.min(axis = 1, keepdims = True))\n",
    "        padFile = np.zeros((159, 19))\n",
    "        file = np.concatenate((file, padFile))\n",
    "        file = file[:160]\n",
    "        \n",
    "        data = np.expand_dims(file, axis=0)\n",
    "        testArr[cnt] = data\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6385, 160, 19)\n",
      "6385\n",
      "(1738, 160, 19)\n",
      "1738\n"
     ]
    }
   ],
   "source": [
    "print(npArr.shape)\n",
    "#print(labelArr.shape)\n",
    "print(len(indxArr))\n",
    "print(testArr.shape)\n",
    "print(len(indyArr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6385, 22)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encode label\n",
    "from keras.utils import to_categorical\n",
    "enc_labelArr = to_categorical(labelArr)\n",
    "enc_labelArr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving and loading\n",
    "np.save('npArr', npArr)\n",
    "np.save('enc_labelArr', enc_labelArr)\n",
    "np.save('indxArr', indxArr)\n",
    "np.save('testArr', testArr)\n",
    "np.save('indyArr', indyArr)\n",
    "\n",
    "\n",
    "\n",
    "#npArr = np.load('npArr.npy')\n",
    "#enc_labelArr = np.load('enc_labelArr.npy')\n",
    "#indxArr = np.load('indxArr.npy')\n",
    "#testArr = np.load('testArr.npy')\n",
    "#indyArr = np.load('indyArr.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[1.47345597e-01, 1.44226724e-01, 1.43087830e-01, ...,\n",
       "         0.00000000e+00, 1.89424208e-09, 1.70055990e-09],\n",
       "        [1.60306486e-01, 1.77104264e-01, 1.61159440e-01, ...,\n",
       "         0.00000000e+00, 1.17674168e-09, 5.64717544e-10],\n",
       "        [1.35275586e-01, 1.50035594e-01, 1.49557060e-01, ...,\n",
       "         0.00000000e+00, 6.81040809e-10, 7.32118870e-10],\n",
       "        ...,\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "       [[6.48820909e-02, 1.18077136e-01, 8.20440598e-02, ...,\n",
       "         1.08087166e-09, 6.13634854e-10, 3.27523261e-09],\n",
       "        [4.69869015e-02, 1.12935200e-01, 5.35557489e-02, ...,\n",
       "         6.09639444e-10, 4.68691372e-10, 1.45073843e-09],\n",
       "        [1.03961372e-01, 1.27691617e-01, 1.09840204e-01, ...,\n",
       "         4.38812800e-10, 4.69099826e-10, 2.14469382e-10],\n",
       "        ...,\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "       [[2.44431167e-01, 2.17942782e-01, 2.00490117e-01, ...,\n",
       "         2.05222951e-09, 1.47427245e-09, 3.36873869e-10],\n",
       "        [1.73641602e-01, 1.67852148e-01, 1.64839774e-01, ...,\n",
       "         2.91103131e-09, 2.07833053e-09, 4.03424978e-10],\n",
       "        [2.01071002e-01, 2.14387607e-01, 1.70096289e-01, ...,\n",
       "         2.38128660e-09, 1.62275587e-09, 4.19768245e-10],\n",
       "        ...,\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2.90616383e-01, 1.00000000e+00, 8.36937200e-02, ...,\n",
       "         1.29204866e-09, 9.33569384e-10, 2.00713337e-10],\n",
       "        [1.00000000e+00, 4.25608447e-01, 1.12313173e-01, ...,\n",
       "         1.78397556e-09, 1.40541685e-09, 0.00000000e+00],\n",
       "        [1.00000000e+00, 6.80378021e-01, 2.92651104e-02, ...,\n",
       "         6.72486657e-10, 5.11509284e-10, 0.00000000e+00],\n",
       "        ...,\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "       [[1.00000000e+00, 7.25850315e-01, 6.33717997e-01, ...,\n",
       "         2.14109837e-08, 8.97518988e-09, 3.91239634e-09],\n",
       "        [1.00000000e+00, 6.83509680e-01, 5.51543406e-01, ...,\n",
       "         1.83182648e-08, 3.61323168e-09, 3.65730623e-09],\n",
       "        [1.00000000e+00, 1.39699632e-01, 9.55088151e-02, ...,\n",
       "         7.41709380e-09, 1.46061033e-09, 3.20463230e-09],\n",
       "        ...,\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "       [[7.89917716e-01, 1.00000000e+00, 7.05570779e-01, ...,\n",
       "         5.03420965e-09, 3.66809682e-09, 0.00000000e+00],\n",
       "        [4.18454223e-01, 1.00000000e+00, 6.77389434e-01, ...,\n",
       "         3.27863905e-09, 2.47501444e-09, 0.00000000e+00],\n",
       "        [5.49848077e-01, 1.00000000e+00, 5.42328880e-01, ...,\n",
       "         2.03448401e-09, 9.02732796e-10, 0.00000000e+00],\n",
       "        ...,\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.isnan(np.sum(npArr)))\n",
    "npArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5108, 160, 19)\n",
      "(5108, 22)\n"
     ]
    }
   ],
   "source": [
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(npArr, enc_labelArr, test_size = 0.2, random_state=7)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Conv Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify hyperparameters\n",
    "num_classes = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create CNN structure\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv1D(32, kernel_size=10, activation='relu', input_shape=(npArr.shape[1],npArr.shape[2])))\n",
    "model.add(layers.SpatialDropout1D(rate=0.2))\n",
    "#model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Conv1D(64, kernel_size=5, activation='relu'))\n",
    "model.add(layers.SpatialDropout1D(rate=0.4))\n",
    "#model.add(layers.BatchNormalization())\n",
    "\n",
    "#model.add(layers.Conv1D(256, kernel_size=4, activation='relu'))\n",
    "#model.add(layers.SpatialDropout1D(rate=0.4))\n",
    "#model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Conv1D(128, kernel_size=4, activation='relu'))\n",
    "model.add(layers.SpatialDropout1D(rate=0.4))\n",
    "#model.add(layers.BatchNormalization())\n",
    "\n",
    "#model.add(layers.Conv1D(64, kernel_size=4, activation='relu'))\n",
    "#model.add(layers.SpatialDropout1D(rate=0.4))\n",
    "#model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(5000, activation = 'relu'))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(num_classes, activation= 'softmax'))\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", patience=10), ModelCheckpoint(filepath=\"best_cnn_model.h5\", \n",
    "                                                                           monitor=\"val_loss\",save_best_only=True)]\n",
    "\n",
    "adam = optimizers.Adam(lr = 0.001, decay = 0.0001)\n",
    "\n",
    "#compile model using accuracy to measure model performance\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5108 samples, validate on 1277 samples\n",
      "Epoch 1/200\n",
      " 416/5108 [=>............................] - ETA: 3:45 - loss: nan - acc: 0.0409"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-d520c81a4e36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m history = model.fit(npArr, enc_labelArr, epochs=epochs, verbose=1, batch_size=batch_size, \n\u001b[0;32m----> 6\u001b[0;31m                       callbacks=callbacks, validation_split = 0.2)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#specify hyperparameters\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(npArr, enc_labelArr, epochs=epochs, verbose=1, batch_size=batch_size, \n",
    "                      callbacks=callbacks, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maybe 2D Conv Model?\n",
    "maybe 1D Conv Layer then a couple 2D Conv?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.6515067e-02, -6.7485064e-02, -4.0974207e-02, ...,\n",
       "         3.4078300e-07, -2.1590799e-06,  0.0000000e+00],\n",
       "       [ 2.1176578e-02, -2.7541181e-02, -4.2534634e-02, ...,\n",
       "        -6.9564326e-06,  4.9421424e-06,  9.0536887e-06],\n",
       "       [ 5.3427730e-02, -1.5425559e-02, -2.5265059e-02, ...,\n",
       "        -1.6132236e-06,  7.0478754e-06,  2.3184088e-05],\n",
       "       [ 5.1594611e-02, -3.2933217e-02, -8.1078112e-02, ...,\n",
       "        -6.1627206e-06, -1.0733088e-06,  7.5541160e-05],\n",
       "       [ 2.4223570e-02, -3.6726356e-02, -3.2027334e-02, ...,\n",
       "         1.7665525e-06,  7.1124373e-07,  3.2856657e-05]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset\n",
    "feature_set = pd.read_csv(dataFolder + 'new_feature_df.csv')\n",
    "feature_set.replace(np.NaN, 0.0, inplace=True)\n",
    "feature_set.replace(np.inf, 4.0, inplace=True)\n",
    "\n",
    "# get trainDf again if top isn't run fully\n",
    "trainDf = pd.read_csv(dataFolder+trainData)\n",
    "trainDf = trainDf[trainDf.Label != 'lay']\n",
    "\n",
    "#merge labels to get training set\n",
    "feature_set = pd.merge(feature_set, trainDf, left_on='id', right_on = 'Datafile', how = 'inner')\n",
    "fileorder = feature_set['id']\n",
    "\n",
    "# one-hot encode label\n",
    "from keras.utils import to_categorical\n",
    "factor = pd.factorize(feature_set.Label)\n",
    "feature_set.Label = factor[0]\n",
    "labels = to_categorical(feature_set['Label'].values)\n",
    "\n",
    "feature_set.drop(columns= ['id', 'Subject', 'Datafile'], inplace = True)\n",
    "\n",
    "\n",
    "#normalize feature set\n",
    "f = feature_set.values.astype('float32')\n",
    "f = normalize(f, axis = 1)\n",
    "f[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create MLP structure\n",
    "mlp = models.Sequential()\n",
    "mlp.add(layers.Dense(2640, activation='relu', input_dim=264))\n",
    "mlp.add(layers.Dropout(rate=0.2))\n",
    "mlp.add(layers.Dense(1280, activation = 'relu'))\n",
    "mlp.add(layers.Dropout(rate=0.5))\n",
    "mlp.add(layers.Dense(800, activation = 'relu'))\n",
    "mlp.add(layers.Dropout(rate=0.5))\n",
    "mlp.add(layers.Dense(400, activation = 'relu'))\n",
    "mlp.add(layers.Dropout(rate=0.5))\n",
    "mlp.add(layers.Dense(22, activation= 'softmax'))\n",
    "\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", patience=10), ModelCheckpoint(filepath=\"best_mlp_model.h5\", \n",
    "                                                                           monitor=\"val_loss\",save_best_only=True)]\n",
    "\n",
    "adam = optimizers.Adam(lr = 0.001)\n",
    "sgd = optimizers.SGD(lr = 0.001, momentum = 0.1, decay = 0.001)\n",
    "#compile model using accuracy to measure model performance\n",
    "mlp.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5108 samples, validate on 1277 samples\n",
      "Epoch 1/200\n",
      "5108/5108 [==============================] - 14s 3ms/step - loss: 2.5054 - acc: 0.1754 - val_loss: 2.0944 - val_acc: 0.2553\n",
      "Epoch 2/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 1.6522 - acc: 0.3604 - val_loss: 1.8482 - val_acc: 0.3547\n",
      "Epoch 3/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 1.2091 - acc: 0.4955 - val_loss: 1.2940 - val_acc: 0.4894\n",
      "Epoch 4/200\n",
      "5108/5108 [==============================] - 12s 2ms/step - loss: 0.9715 - acc: 0.5781 - val_loss: 1.3999 - val_acc: 0.5129\n",
      "Epoch 5/200\n",
      "5108/5108 [==============================] - 12s 2ms/step - loss: 0.8481 - acc: 0.6341 - val_loss: 1.1263 - val_acc: 0.5873\n",
      "Epoch 6/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.8224 - acc: 0.6443 - val_loss: 1.4963 - val_acc: 0.5411\n",
      "Epoch 7/200\n",
      "5108/5108 [==============================] - 12s 2ms/step - loss: 0.6800 - acc: 0.7034 - val_loss: 1.2380 - val_acc: 0.5442\n",
      "Epoch 8/200\n",
      "5108/5108 [==============================] - 12s 2ms/step - loss: 0.6598 - acc: 0.7120 - val_loss: 1.2010 - val_acc: 0.5818\n",
      "Epoch 9/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.6350 - acc: 0.7306 - val_loss: 1.1419 - val_acc: 0.6006\n",
      "Epoch 10/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.5788 - acc: 0.7500 - val_loss: 1.2274 - val_acc: 0.5834\n",
      "Epoch 11/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.5763 - acc: 0.7580 - val_loss: 1.3360 - val_acc: 0.5779\n",
      "Epoch 12/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.5505 - acc: 0.7657 - val_loss: 1.1067 - val_acc: 0.6406\n",
      "Epoch 13/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.5074 - acc: 0.7884 - val_loss: 1.0319 - val_acc: 0.6547\n",
      "Epoch 14/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.5138 - acc: 0.7864 - val_loss: 1.2575 - val_acc: 0.5873\n",
      "Epoch 15/200\n",
      "5108/5108 [==============================] - 12s 2ms/step - loss: 0.4836 - acc: 0.7962 - val_loss: 1.2071 - val_acc: 0.6030\n",
      "Epoch 16/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.4503 - acc: 0.8125 - val_loss: 1.1345 - val_acc: 0.6359\n",
      "Epoch 17/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.4207 - acc: 0.8332 - val_loss: 1.2094 - val_acc: 0.6202\n",
      "Epoch 18/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.4225 - acc: 0.8356 - val_loss: 1.2694 - val_acc: 0.6116\n",
      "Epoch 19/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.4152 - acc: 0.8285 - val_loss: 1.0938 - val_acc: 0.6617\n",
      "Epoch 20/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.3928 - acc: 0.8457 - val_loss: 1.1286 - val_acc: 0.6460\n",
      "Epoch 21/200\n",
      "5108/5108 [==============================] - 12s 2ms/step - loss: 0.3669 - acc: 0.8549 - val_loss: 1.1180 - val_acc: 0.6421\n",
      "Epoch 22/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.3790 - acc: 0.8532 - val_loss: 1.2205 - val_acc: 0.6351\n",
      "Epoch 23/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.3488 - acc: 0.8610 - val_loss: 1.1110 - val_acc: 0.6609\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "hist = mlp.fit(f, labels, batch_size=batch_size, epochs=epochs, callbacks=callbacks, \n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN plus feature MLP\n",
    "inspired by https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "feature_set = pd.read_csv(dataFolder + 'new_feature_df.csv')\n",
    "feature_set.replace(np.NaN, 0.0, inplace=True)\n",
    "feature_set.replace(np.inf, 4.0, inplace=True)\n",
    "\n",
    "# get trainDf again if top isn't run fully\n",
    "trainDf = pd.read_csv(dataFolder+trainData)\n",
    "trainDf = trainDf[trainDf.Label != 'lay']\n",
    "\n",
    "#merge labels to get training set\n",
    "feature_set = pd.merge(feature_set, trainDf, left_on='id', right_on = 'Datafile', how = 'inner')\n",
    "fileorder = feature_set['id']\n",
    "\n",
    "feature_set.drop(columns= ['id', 'Subject', 'Datafile', 'Label'], inplace = True)\n",
    "\n",
    "#normalize feature set\n",
    "f = feature_set.values\n",
    "f = (f - f.min(axis=1, keepdims=True)) / (f.max(axis=1, keepdims=True) - f.min(axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Network Structure\n",
    "# import the necessary packages\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import concatenate\n",
    "\n",
    "#input_ts = Input(shape=(npArr.shape[1], npArr.shape[2]))\n",
    "#input_f = Input(shape=f_train.shape[1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/BBDC-GTA/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/BBDC-GTA/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "## creating CNN component\n",
    "# initialize the input shape and channel dimension, assuming\n",
    "# TensorFlow/channels-last ordering\n",
    "    \n",
    "inputShape = (npArr.shape[1], npArr.shape[2],)\n",
    "chanDim = -1\n",
    " \n",
    "# define the model input\n",
    "input_ts = Input(shape=inputShape)\n",
    " \n",
    "# CONV => RELU => BN => POOL\n",
    "c = Conv1D(32, 10, strides=10, activation = 'relu')(input_ts)\n",
    "c = BatchNormalization(axis=chanDim, momentum = 0.0)(c)\n",
    "c = MaxPooling1D(pool_size=2)(c)\n",
    "c = SpatialDropout1D(rate=0.2)(c)\n",
    "    \n",
    "c = Conv1D(64, 10, activation = 'relu')(c)\n",
    "c = BatchNormalization(axis=chanDim, momentum = 0.0)(c)\n",
    "c = MaxPooling1D(pool_size=2)(c)\n",
    "c = SpatialDropout1D(rate=0.2)(c)\n",
    "    \n",
    "c = Conv1D(128, 4, activation = 'relu')(c)\n",
    "c = BatchNormalization(axis=chanDim, momentum = 0.0)(c)\n",
    "c = MaxPooling1D(pool_size=2)(c)\n",
    "c = SpatialDropout1D(rate=0.2)(c)\n",
    "        \n",
    "# flatten the volume, then FC => RELU => BN => DROPOUT\n",
    "c = Flatten()(c)\n",
    "c = Dense(2000, activation='relu')(c)\n",
    "c = Dropout(0.4)(c)\n",
    " \n",
    "# apply another FC layer, this one to match the number of nodes\n",
    "# coming out of the MLP\n",
    "c_out = Dense(500, activation='relu')(c)\n",
    "\n",
    "### create simple MLP\n",
    "#model input\n",
    "input_m = Input(shape=(264,))\n",
    "\n",
    "#two dense layers with dropout in between\n",
    "m = Dense(1000, activation=\"relu\")(input_m)\n",
    "m = BatchNormalization(axis=chanDim, momentum = 0.0)(m)\n",
    "m = Dropout(rate=0.2)(m)\n",
    "m = Dense(1000, activation=\"relu\")(input_m)\n",
    "m = Dropout(rate=0.4)(m)\n",
    "m_out = Dense(500, activation=\"relu\")(m)\n",
    "\n",
    "# create the input to our final set of layers as the *output* of both\n",
    "# the MLP and CNN\n",
    "comb = concatenate([c_out, m_out])\n",
    "\n",
    "# our final FC layer head will have two dense layers, the final one\n",
    "# being our regression head\n",
    "comb = Dense(1000, activation=\"relu\")(comb)\n",
    "comb = Dropout(rate = 0.5)(comb)\n",
    "out = Dense(22, activation=\"softmax\")(comb)\n",
    "\n",
    "model = Model(inputs=[input_ts, input_m], outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify optimizers\n",
    "adam = Adam(lr = 0.001, decay = 0.001 / 200)\n",
    "\n",
    "#compile model using accuracy to measure model performance\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/BBDC-GTA/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 5108 samples, validate on 1277 samples\n",
      "Epoch 1/200\n",
      "5108/5108 [==============================] - 65s 13ms/step - loss: 2.5863 - acc: 0.2101 - val_loss: 1.3896 - val_acc: 0.4839\n",
      "Epoch 2/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 1.3617 - acc: 0.5305 - val_loss: 0.9725 - val_acc: 0.6711\n",
      "Epoch 3/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.8792 - acc: 0.6962 - val_loss: 0.7243 - val_acc: 0.7439\n",
      "Epoch 4/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.6472 - acc: 0.7892 - val_loss: 0.6493 - val_acc: 0.7886\n",
      "Epoch 5/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.5158 - acc: 0.8289 - val_loss: 0.8706 - val_acc: 0.7619\n",
      "Epoch 6/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.4650 - acc: 0.8496 - val_loss: 0.8612 - val_acc: 0.7776\n",
      "Epoch 7/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.4056 - acc: 0.8682 - val_loss: 0.6081 - val_acc: 0.8168\n",
      "Epoch 8/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.3610 - acc: 0.8821 - val_loss: 0.7228 - val_acc: 0.8215\n",
      "Epoch 9/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.3470 - acc: 0.8923 - val_loss: 0.7292 - val_acc: 0.8097\n",
      "Epoch 10/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.2976 - acc: 0.8970 - val_loss: 0.7127 - val_acc: 0.8081\n",
      "Epoch 11/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.3255 - acc: 0.8970 - val_loss: 0.8164 - val_acc: 0.7925\n",
      "Epoch 12/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.2768 - acc: 0.9164 - val_loss: 0.8801 - val_acc: 0.7659\n",
      "Epoch 13/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.2803 - acc: 0.9233 - val_loss: 0.8262 - val_acc: 0.8081\n",
      "Epoch 14/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.2288 - acc: 0.9309 - val_loss: 0.7978 - val_acc: 0.8066\n",
      "Epoch 15/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.2290 - acc: 0.9301 - val_loss: 0.7547 - val_acc: 0.8277\n",
      "Epoch 16/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.2031 - acc: 0.9330 - val_loss: 0.9535 - val_acc: 0.7956\n",
      "Epoch 17/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.1898 - acc: 0.9424 - val_loss: 0.7214 - val_acc: 0.8285\n",
      "Epoch 18/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.2060 - acc: 0.9383 - val_loss: 0.9019 - val_acc: 0.8363\n",
      "Epoch 19/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.1505 - acc: 0.9499 - val_loss: 0.7710 - val_acc: 0.8403\n",
      "Epoch 20/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.2082 - acc: 0.9417 - val_loss: 0.7405 - val_acc: 0.8191\n",
      "Epoch 21/200\n",
      "1088/5108 [=====>........................] - ETA: 46s - loss: 0.1818 - acc: 0.9522"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8a30d0b698c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m model.fit([npArr, f], enc_labelArr, batch_size=batch_size, epochs=epochs, callbacks=callbacks, \n\u001b[0;32m---> 10\u001b[0;31m           validation_split=0.2, shuffle = True)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#specify hyperparameters\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "#early stopping\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", patience=30), ModelCheckpoint(filepath=\"best_comb_model.h5\", \n",
    "                                                                           monitor=\"val_loss\",save_best_only=True)]\n",
    "\n",
    "model.fit([npArr, f], enc_labelArr, batch_size=batch_size, epochs=epochs, callbacks=callbacks, \n",
    "          validation_split=0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning sensor time series into graphs and running CNN on the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>n_rows</th>\n",
       "      <th>emg1_min</th>\n",
       "      <th>emg2_min</th>\n",
       "      <th>emg3_min</th>\n",
       "      <th>emg4_min</th>\n",
       "      <th>airborne_min</th>\n",
       "      <th>acc_u_x_min</th>\n",
       "      <th>acc_u_y_min</th>\n",
       "      <th>acc_u_z_min</th>\n",
       "      <th>...</th>\n",
       "      <th>cor_gyro_u_y_gyro_u_z</th>\n",
       "      <th>cor_gyro_u_y_gyro_l_x</th>\n",
       "      <th>cor_gyro_u_y_gyro_l_y</th>\n",
       "      <th>cor_gyro_u_y_gyro_l_z</th>\n",
       "      <th>cor_gyro_u_z_gyro_l_x</th>\n",
       "      <th>cor_gyro_u_z_gyro_l_y</th>\n",
       "      <th>cor_gyro_u_z_gyro_l_z</th>\n",
       "      <th>cor_gyro_l_x_gyro_l_y</th>\n",
       "      <th>cor_gyro_l_x_gyro_l_z</th>\n",
       "      <th>cor_gyro_l_y_gyro_l_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject05/Subject05_Aufnahme328.csv</td>\n",
       "      <td>1879</td>\n",
       "      <td>-7678.106440</td>\n",
       "      <td>-4661.836615</td>\n",
       "      <td>-3048.633316</td>\n",
       "      <td>-7512.425226</td>\n",
       "      <td>-6532.705695</td>\n",
       "      <td>-6284.303353</td>\n",
       "      <td>-6827.939329</td>\n",
       "      <td>-16398.609367</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.105410</td>\n",
       "      <td>0.253597</td>\n",
       "      <td>-0.779391</td>\n",
       "      <td>0.084289</td>\n",
       "      <td>-0.158099</td>\n",
       "      <td>0.147542</td>\n",
       "      <td>0.310263</td>\n",
       "      <td>-0.257414</td>\n",
       "      <td>0.038773</td>\n",
       "      <td>-0.245649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject05/Subject05_Aufnahme171.csv</td>\n",
       "      <td>2339</td>\n",
       "      <td>-3041.984609</td>\n",
       "      <td>-4698.044891</td>\n",
       "      <td>-9783.315092</td>\n",
       "      <td>-9683.179991</td>\n",
       "      <td>-26136.890552</td>\n",
       "      <td>-6783.916204</td>\n",
       "      <td>-3552.906370</td>\n",
       "      <td>-7639.485250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014532</td>\n",
       "      <td>0.021924</td>\n",
       "      <td>-0.099626</td>\n",
       "      <td>-0.106292</td>\n",
       "      <td>-0.809414</td>\n",
       "      <td>0.554187</td>\n",
       "      <td>0.849509</td>\n",
       "      <td>-0.389958</td>\n",
       "      <td>-0.768353</td>\n",
       "      <td>0.545871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject05/Subject05_Aufnahme075.csv</td>\n",
       "      <td>4609</td>\n",
       "      <td>-1330.702322</td>\n",
       "      <td>-2179.517032</td>\n",
       "      <td>-4327.324365</td>\n",
       "      <td>-5921.050770</td>\n",
       "      <td>-18257.753309</td>\n",
       "      <td>-2382.916468</td>\n",
       "      <td>-2659.078325</td>\n",
       "      <td>-2702.537644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040820</td>\n",
       "      <td>-0.340181</td>\n",
       "      <td>-0.094396</td>\n",
       "      <td>-0.290452</td>\n",
       "      <td>-0.254656</td>\n",
       "      <td>0.746048</td>\n",
       "      <td>0.724259</td>\n",
       "      <td>-0.186918</td>\n",
       "      <td>-0.139166</td>\n",
       "      <td>0.607992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject05/Subject05_Aufnahme422.csv</td>\n",
       "      <td>2049</td>\n",
       "      <td>-1307.891654</td>\n",
       "      <td>-3219.891654</td>\n",
       "      <td>-2915.438263</td>\n",
       "      <td>-1877.487555</td>\n",
       "      <td>-3637.914592</td>\n",
       "      <td>-3448.846266</td>\n",
       "      <td>-1746.390434</td>\n",
       "      <td>-6552.304539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264991</td>\n",
       "      <td>0.152688</td>\n",
       "      <td>0.157523</td>\n",
       "      <td>0.106624</td>\n",
       "      <td>-0.169769</td>\n",
       "      <td>0.270967</td>\n",
       "      <td>-0.013500</td>\n",
       "      <td>0.115817</td>\n",
       "      <td>-0.244743</td>\n",
       "      <td>-0.042625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject05/Subject05_Aufnahme407.csv</td>\n",
       "      <td>2949</td>\n",
       "      <td>-4471.100373</td>\n",
       "      <td>-3899.037640</td>\n",
       "      <td>-3192.797898</td>\n",
       "      <td>-9610.212614</td>\n",
       "      <td>-26111.219736</td>\n",
       "      <td>-7406.349949</td>\n",
       "      <td>-7765.860292</td>\n",
       "      <td>-7300.351306</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.468957</td>\n",
       "      <td>-0.166220</td>\n",
       "      <td>-0.675509</td>\n",
       "      <td>-0.223699</td>\n",
       "      <td>0.178065</td>\n",
       "      <td>0.379014</td>\n",
       "      <td>0.579796</td>\n",
       "      <td>-0.310766</td>\n",
       "      <td>0.215062</td>\n",
       "      <td>0.086587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 265 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id  n_rows     emg1_min     emg2_min  \\\n",
       "0  Subject05/Subject05_Aufnahme328.csv    1879 -7678.106440 -4661.836615   \n",
       "1  Subject05/Subject05_Aufnahme171.csv    2339 -3041.984609 -4698.044891   \n",
       "2  Subject05/Subject05_Aufnahme075.csv    4609 -1330.702322 -2179.517032   \n",
       "3  Subject05/Subject05_Aufnahme422.csv    2049 -1307.891654 -3219.891654   \n",
       "4  Subject05/Subject05_Aufnahme407.csv    2949 -4471.100373 -3899.037640   \n",
       "\n",
       "      emg3_min     emg4_min  airborne_min  acc_u_x_min  acc_u_y_min  \\\n",
       "0 -3048.633316 -7512.425226  -6532.705695 -6284.303353 -6827.939329   \n",
       "1 -9783.315092 -9683.179991 -26136.890552 -6783.916204 -3552.906370   \n",
       "2 -4327.324365 -5921.050770 -18257.753309 -2382.916468 -2659.078325   \n",
       "3 -2915.438263 -1877.487555  -3637.914592 -3448.846266 -1746.390434   \n",
       "4 -3192.797898 -9610.212614 -26111.219736 -7406.349949 -7765.860292   \n",
       "\n",
       "    acc_u_z_min  ...  cor_gyro_u_y_gyro_u_z  cor_gyro_u_y_gyro_l_x  \\\n",
       "0 -16398.609367  ...              -0.105410               0.253597   \n",
       "1  -7639.485250  ...               0.014532               0.021924   \n",
       "2  -2702.537644  ...               0.040820              -0.340181   \n",
       "3  -6552.304539  ...               0.264991               0.152688   \n",
       "4  -7300.351306  ...              -0.468957              -0.166220   \n",
       "\n",
       "   cor_gyro_u_y_gyro_l_y  cor_gyro_u_y_gyro_l_z  cor_gyro_u_z_gyro_l_x  \\\n",
       "0              -0.779391               0.084289              -0.158099   \n",
       "1              -0.099626              -0.106292              -0.809414   \n",
       "2              -0.094396              -0.290452              -0.254656   \n",
       "3               0.157523               0.106624              -0.169769   \n",
       "4              -0.675509              -0.223699               0.178065   \n",
       "\n",
       "   cor_gyro_u_z_gyro_l_y  cor_gyro_u_z_gyro_l_z  cor_gyro_l_x_gyro_l_y  \\\n",
       "0               0.147542               0.310263              -0.257414   \n",
       "1               0.554187               0.849509              -0.389958   \n",
       "2               0.746048               0.724259              -0.186918   \n",
       "3               0.270967              -0.013500               0.115817   \n",
       "4               0.379014               0.579796              -0.310766   \n",
       "\n",
       "   cor_gyro_l_x_gyro_l_z  cor_gyro_l_y_gyro_l_z  \n",
       "0               0.038773              -0.245649  \n",
       "1              -0.768353               0.545871  \n",
       "2              -0.139166               0.607992  \n",
       "3              -0.244743              -0.042625  \n",
       "4               0.215062               0.086587  \n",
       "\n",
       "[5 rows x 265 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load feature df to look at mins/maxs for images\n",
    "feature_df = pd.read_csv(mainFolder + 'new_feature_df.csv')\n",
    "\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'n_rows', 'emg1_min', 'emg2_min', 'emg3_min', 'emg4_min',\n",
       "       'airborne_min', 'acc_u_x_min', 'acc_u_y_min', 'acc_u_z_min',\n",
       "       'gonio_x_min', 'acc_l_x_min', 'acc_l_y_min', 'acc_l_z_min',\n",
       "       'gonio_y_min', 'gyro_u_x_min', 'gyro_u_y_min', 'gyro_u_z_min',\n",
       "       'gyro_l_x_min', 'gyro_l_y_min', 'gyro_l_z_min', 'emg1_max', 'emg2_max',\n",
       "       'emg3_max', 'emg4_max', 'airborne_max', 'acc_u_x_max', 'acc_u_y_max',\n",
       "       'acc_u_z_max', 'gonio_x_max', 'acc_l_x_max', 'acc_l_y_max',\n",
       "       'acc_l_z_max', 'gonio_y_max', 'gyro_u_x_max', 'gyro_u_y_max',\n",
       "       'gyro_u_z_max', 'gyro_l_x_max', 'gyro_l_y_max', 'gyro_l_z_max',\n",
       "       'emg1_std', 'emg2_std', 'emg3_std', 'emg4_std', 'airborne_std',\n",
       "       'acc_u_x_std', 'acc_u_y_std', 'acc_u_z_std', 'gonio_x_std',\n",
       "       'acc_l_x_std'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df.columns[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33941.816560894535,\n",
       " 32833.65760555235,\n",
       " 32854.93024585477,\n",
       " 32973.58286629304,\n",
       " 52745.637340496985,\n",
       " 22873.599682287528,\n",
       " 21567.1392659628,\n",
       " 23893.306845380568,\n",
       " 10979.977144998127,\n",
       " 37023.81536293164,\n",
       " 38556.35982339956,\n",
       " 37217.67458603312,\n",
       " 10942.669613773847,\n",
       " 37525.20099255583,\n",
       " 39835.59878154917,\n",
       " 40473.19206492335,\n",
       " 37637.143934201515,\n",
       " 40257.900990099006,\n",
       " 39148.57755539671]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_list = list(feature_df.columns[21:40])\n",
    "max_values = list(feature_df[max_list].max())\n",
    "max_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-32915.482561463694,\n",
       " -33119.311906092786,\n",
       " -32868.74304322084,\n",
       " -62068.8383862497,\n",
       " -32972.93692178301,\n",
       " -25044.719731625497,\n",
       " -28415.224992490235,\n",
       " -27177.726172912364,\n",
       " -9310.37669059943,\n",
       " -29776.421763304214,\n",
       " -31110.498317528294,\n",
       " -30268.897595034912,\n",
       " -11102.855388813095,\n",
       " -32316.157631359467,\n",
       " -32177.498635122836,\n",
       " -33929.99018645731,\n",
       " -34299.19911012236,\n",
       " -33714.875364735315,\n",
       " -32748.26306465899]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a list of all columns that contain minimums for all observations\n",
    "min_list = list(feature_df.columns[2:21])\n",
    "min_values = list(feature_df[min_list].min())\n",
    "min_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptation of Prateeks code for generating dataset\n",
    "\n",
    "# Number of files to load\n",
    "numFiles = 6385\n",
    "\n",
    "# Maximum file size\n",
    "maxlen = 8000\n",
    "\n",
    "#hyperparams\n",
    "window_length = 100\n",
    "# n_windows = 8000 / window_length\n",
    "n_windows = 80\n",
    "# number of intervals for binning\n",
    "n_bins = 500\n",
    "\n",
    "# NP Arrays to store the data and the label\n",
    "labelArr = np.empty((numFiles, 1))\n",
    "imgArr = np.empty((numFiles, n_bins, n_windows, 19))\n",
    "indexArr = []\n",
    "\n",
    "# read the training data into a dataframe\n",
    "trainDf = pd.read_csv(dataFolder+trainData)\n",
    "trainDf = trainDf[trainDf.Label != 'lay']\n",
    "# factorize the labels\n",
    "factor = pd.factorize(trainDf.Label)\n",
    "trainDf.Label = factor[0]\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for eachFile in result:\n",
    "  # Only read the dataset with labels available\n",
    "    if sum(str(eachFile)[-35:] == trainDf.Datafile)>0:\n",
    "        indexArr.append(str(eachFile)[-35:])\n",
    "        file = np.loadtxt(str(eachFile), delimiter=',')\n",
    "        #min max scale the input\n",
    "        \n",
    "        \n",
    "        #zero pad to conserve last values\n",
    "        padFile = np.zeros((8000, 19))\n",
    "        file = np.concatenate((file, padFile))\n",
    "        file = file[:8000,:]\n",
    "        \n",
    "        #creates windows by reshaping, creates 80 windows of 100 observations each\n",
    "        file = file.reshape(n_windows, window_length, 19)\n",
    "        \n",
    "        #initializing empty binned file\n",
    "        bfile = np.empty((n_bins, n_windows, 19))\n",
    "        \n",
    "        #loop over sensors and windows to put each observation into a bin for each window\n",
    "        for sens in range(file.shape[2]):\n",
    "            #create interval index for bins from min and max values\n",
    "            bins = pd.cut(np.array([min_values[sens], max_values[sens]]), bins = bfile.shape[0]).categories\n",
    "            for window in range(file.shape[0]):\n",
    "                #get the counts for each bin in descending order and insert into new file\n",
    "                bfile[:,window,sens] = list(pd.cut(file[window,:,sens], bins=bins).value_counts())[::-1]\n",
    "        \n",
    "        #normalize bfile by dividing all counts by 100 (n_obs in each window)\n",
    "        bfile = bfile / window_length\n",
    "        data = np.expand_dims(bfile, axis=0)\n",
    "        imgArr[cnt] = data\n",
    "        labelArr[cnt] = trainDf[trainDf.Datafile == str(eachFile)[-35:]].Label\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgArr = npArr\n",
    "indexArr = indxArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6385, 500, 80, 19)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgArr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('imgArr_500b_80w', imgArr)\n",
    "np.save('labelArr', labelArrd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgArr = np.load('imgArr_500b_80w.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6385, 22)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encode label\n",
    "from keras.utils import to_categorical\n",
    "enc_labelArr = to_categorical(labelArr)\n",
    "enc_labelArr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(np.isnan(np.sum(imgArr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Creation comments\n",
    "Next time initialize bfile as zeros and don't do any padding, will just fill the entries that the file has. Have to look further into looping and such but would save time when creating dataset <br>\n",
    "Also should either do smaller bins or larger windows so dimensions are somewhat similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages for the NN structure\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import SpatialDropout2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputShape = (imgArr.shape[1], imgArr.shape[2], imgArr.shape[3])\n",
    "chanDim = -1\n",
    " \n",
    "# define the model input\n",
    "imginput = Input(shape=inputShape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONV => RELU => BN => POOL\n",
    "cnn = Conv2D(32, (10, 4), activation = 'relu', use_bias=True)(imginput)\n",
    "#cnn = BatchNormalization(axis=chanDim)(cnn)\n",
    "#cnn = SpatialDropout2D(rate=0.2)(cnn)\n",
    "    \n",
    "cnn = Conv2D(64, 4, activation = 'relu', use_bias = True)(cnn)\n",
    "#cnn = BatchNormalization(axis=chanDim)(cnn)\n",
    "#cnn = SpatialDropout1D(rate=0.2)(cnn)\n",
    "    \n",
    "cnn = Conv2D(32, 4, activation = 'relu', use_bias=True)(cnn)\n",
    "#cnn = BatchNormalization(axis=chanDim)(cnn)\n",
    "#cnn = SpatialDropout1D(rate=0.2)(cnn)\n",
    "        \n",
    "# flatten it for predictions\n",
    "cnn = Flatten()(cnn)\n",
    "cnn = Dense(2000, activation='relu')(cnn)\n",
    "#cnn = Dropout(0.4)(cnn)\n",
    "out = Dense(22, activation='softmax')(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(lr = 0.001, decay = 0.0000001)\n",
    "\n",
    "#compile model using accuracy to measure model performance\n",
    "model = Model(inputs=imginput, outputs=out)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/markkoerner/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 5108 samples, validate on 1277 samples\n",
      "Epoch 1/200\n"
     ]
    }
   ],
   "source": [
    "#specify hyperparameters\n",
    "epochs = 200\n",
    "batch_size = 100\n",
    "\n",
    "#early stopping\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", patience=10), ModelCheckpoint(filepath=\"best_img_model.h5\", \n",
    "                                                                           monitor=\"val_loss\",save_best_only=True)]\n",
    "\n",
    "model.fit(imgArr, enc_labelArr, batch_size=batch_size, epochs=epochs, callbacks=callbacks, \n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
