{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.utils import normalize\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Prateeks NN notebook\n",
    "\n",
    "# set the variables for the parent data folder and train file\n",
    "dataFolder = '/home/BBDC-GTA/bbdc_2019_Bewegungsdaten/'\n",
    "trainData = 'train.csv'\n",
    "\n",
    "# make a list of all the csv files in the parent and sub directories\n",
    "result = list(Path(dataFolder).rglob(\"Subject*.[cC][sS][vV]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptation of Prateeks code for generating dataset\n",
    "\n",
    "# Number of files to load\n",
    "numFiles = 6385\n",
    "\n",
    "# Maximum file size\n",
    "maxlen = 8000\n",
    "\n",
    "# NP Arrays to store the data and the label\n",
    "labelArr = np.empty((numFiles, 1))\n",
    "npArr = np.empty((numFiles, 160, 19))\n",
    "indxArr = []\n",
    "\n",
    "# read the training data into a dataframe\n",
    "trainDf = pd.read_csv(dataFolder+trainData)\n",
    "trainDf = trainDf[trainDf.Label != 'lay']\n",
    "# factorize the labels\n",
    "factor = pd.factorize(trainDf.Label)\n",
    "trainDf.Label = factor[0]\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for eachFile in result:\n",
    "  # Only read the dataset with labels available\n",
    "    if sum(str(eachFile)[-35:] == trainDf.Datafile)>0:\n",
    "        indxArr.append(str(eachFile)[-35:])\n",
    "        file = np.loadtxt(str(eachFile), delimiter=',')\n",
    "        #min max scale the inpuit\n",
    "        file = (file - file.mean(axis=1, keepdims=True))\n",
    "        \n",
    "        #zero pad to conserve last values\n",
    "        #padFile = np.zeros((50, 19))\n",
    "        #file = np.concatenate((file, padFile))\n",
    "        \n",
    "        #windowing\n",
    "        df = pd.DataFrame(file)\n",
    "        emg = df.iloc[:, :4]\n",
    "        other = df.iloc[::10, 4:]\n",
    "        emg = emg ** 2\n",
    "        emg = emg.rolling(100).mean().dropna(axis = 0, how = 'any').reset_index(drop = True)\n",
    "        emg = emg.iloc[::50, :].reset_index(drop = True)\n",
    "        emg = emg ** 2\n",
    "        other = other.rolling(10).mean().dropna(axis = 0, how ='any').iloc[::5, :].reset_index(drop = True)\n",
    "        df = pd.concat([emg, other], axis = 'columns')\n",
    "        \n",
    "        file = df.values\n",
    "        #extend to max length\n",
    "        file = (file - file.min(axis=1, keepdims=True)) / (file.max(axis = 1, keepdims = True) - file.min(axis = 1, keepdims = True))\n",
    "        padFile = np.zeros((159, 19))\n",
    "        file = np.concatenate((file, padFile))\n",
    "        file = file[:160]\n",
    "        \n",
    "        data = np.expand_dims(file, axis=0)\n",
    "        npArr[cnt] = data\n",
    "        labelArr[cnt] = trainDf[trainDf.Datafile == str(eachFile)[-35:]].Label\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create test array\n",
    "\n",
    "# Maximum file size\n",
    "maxlen = 8000\n",
    "\n",
    "# read the training data into a dataframe\n",
    "testDf = pd.read_csv(dataFolder+'challenge.csv')\n",
    "\n",
    "#number of files to read\n",
    "numFiles = testDf.shape[0]\n",
    "\n",
    "testArr = np.empty((numFiles, 160, 19))\n",
    "indyArr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for eachFile in result:\n",
    "  # Only read the dataset with labels available\n",
    "    if sum(str(eachFile)[-35:] == testDf.Datafile)>0:\n",
    "        indyArr.append(str(eachFile)[-35:])\n",
    "        file = np.loadtxt(str(eachFile), delimiter=',')\n",
    "        #min max scale the inpuit\n",
    "        file = (file - file.mean(axis=1, keepdims=True))\n",
    "        \n",
    "        #zero pad to conserve last values\n",
    "        padFile = np.zeros((50, 19))\n",
    "        file = np.concatenate((file, padFile))\n",
    "        \n",
    "        #windowing\n",
    "        df = pd.DataFrame(file)\n",
    "        emg = df.iloc[:, :4]\n",
    "        other = df.iloc[::10, 4:]\n",
    "        emg = emg ** 2\n",
    "        emg = emg.rolling(100).mean().dropna(axis = 'index').iloc[::50, :].reset_index(drop = True)\n",
    "        other = other.rolling(10).mean().dropna(axis = 'index').iloc[::5, :].reset_index(drop = True)\n",
    "        df = pd.concat([emg, other], axis = 1)\n",
    "        \n",
    "        file = df.values\n",
    "        #extend to max length\n",
    "        file = (file - file.min(axis=1, keepdims=True)) / (file.max(axis = 1, keepdims = True) - file.min(axis = 1, keepdims = True))\n",
    "        padFile = np.zeros((159, 19))\n",
    "        file = np.concatenate((file, padFile))\n",
    "        file = file[:160]\n",
    "        \n",
    "        data = np.expand_dims(file, axis=0)\n",
    "        testArr[cnt] = data\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6385, 160, 19)\n",
      "6385\n",
      "(1738, 160, 19)\n",
      "1738\n"
     ]
    }
   ],
   "source": [
    "print(npArr.shape)\n",
    "#print(labelArr.shape)\n",
    "print(len(indxArr))\n",
    "print(testArr.shape)\n",
    "print(len(indyArr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6385, 22)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encode label\n",
    "from keras.utils import to_categorical\n",
    "enc_labelArr = to_categorical(labelArr)\n",
    "enc_labelArr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving and loading\n",
    "np.save('npArr', npArr)\n",
    "np.save('enc_labelArr', enc_labelArr)\n",
    "np.save('indxArr', indxArr)\n",
    "np.save('testArr', testArr)\n",
    "np.save('indyArr', indyArr)\n",
    "\n",
    "\n",
    "\n",
    "#npArr = np.load('npArr.npy')\n",
    "#enc_labelArr = np.load('enc_labelArr.npy')\n",
    "#indxArr = np.load('indxArr.npy')\n",
    "#testArr = np.load('testArr.npy')\n",
    "#indyArr = np.load('indyArr.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[1.47345597e-01, 1.44226724e-01, 1.43087830e-01, ...,\n",
       "         0.00000000e+00, 1.89424208e-09, 1.70055990e-09],\n",
       "        [1.60306486e-01, 1.77104264e-01, 1.61159440e-01, ...,\n",
       "         0.00000000e+00, 1.17674168e-09, 5.64717544e-10],\n",
       "        [1.35275586e-01, 1.50035594e-01, 1.49557060e-01, ...,\n",
       "         0.00000000e+00, 6.81040809e-10, 7.32118870e-10],\n",
       "        ...,\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "       [[6.48820909e-02, 1.18077136e-01, 8.20440598e-02, ...,\n",
       "         1.08087166e-09, 6.13634854e-10, 3.27523261e-09],\n",
       "        [4.69869015e-02, 1.12935200e-01, 5.35557489e-02, ...,\n",
       "         6.09639444e-10, 4.68691372e-10, 1.45073843e-09],\n",
       "        [1.03961372e-01, 1.27691617e-01, 1.09840204e-01, ...,\n",
       "         4.38812800e-10, 4.69099826e-10, 2.14469382e-10],\n",
       "        ...,\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "       [[2.44431167e-01, 2.17942782e-01, 2.00490117e-01, ...,\n",
       "         2.05222951e-09, 1.47427245e-09, 3.36873869e-10],\n",
       "        [1.73641602e-01, 1.67852148e-01, 1.64839774e-01, ...,\n",
       "         2.91103131e-09, 2.07833053e-09, 4.03424978e-10],\n",
       "        [2.01071002e-01, 2.14387607e-01, 1.70096289e-01, ...,\n",
       "         2.38128660e-09, 1.62275587e-09, 4.19768245e-10],\n",
       "        ...,\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2.90616383e-01, 1.00000000e+00, 8.36937200e-02, ...,\n",
       "         1.29204866e-09, 9.33569384e-10, 2.00713337e-10],\n",
       "        [1.00000000e+00, 4.25608447e-01, 1.12313173e-01, ...,\n",
       "         1.78397556e-09, 1.40541685e-09, 0.00000000e+00],\n",
       "        [1.00000000e+00, 6.80378021e-01, 2.92651104e-02, ...,\n",
       "         6.72486657e-10, 5.11509284e-10, 0.00000000e+00],\n",
       "        ...,\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "       [[1.00000000e+00, 7.25850315e-01, 6.33717997e-01, ...,\n",
       "         2.14109837e-08, 8.97518988e-09, 3.91239634e-09],\n",
       "        [1.00000000e+00, 6.83509680e-01, 5.51543406e-01, ...,\n",
       "         1.83182648e-08, 3.61323168e-09, 3.65730623e-09],\n",
       "        [1.00000000e+00, 1.39699632e-01, 9.55088151e-02, ...,\n",
       "         7.41709380e-09, 1.46061033e-09, 3.20463230e-09],\n",
       "        ...,\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "       [[7.89917716e-01, 1.00000000e+00, 7.05570779e-01, ...,\n",
       "         5.03420965e-09, 3.66809682e-09, 0.00000000e+00],\n",
       "        [4.18454223e-01, 1.00000000e+00, 6.77389434e-01, ...,\n",
       "         3.27863905e-09, 2.47501444e-09, 0.00000000e+00],\n",
       "        [5.49848077e-01, 1.00000000e+00, 5.42328880e-01, ...,\n",
       "         2.03448401e-09, 9.02732796e-10, 0.00000000e+00],\n",
       "        ...,\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.isnan(np.sum(npArr)))\n",
    "npArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5108, 160, 19)\n",
      "(5108, 22)\n"
     ]
    }
   ],
   "source": [
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(npArr, enc_labelArr, test_size = 0.2, random_state=7)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Conv Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify hyperparameters\n",
    "num_classes = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create CNN structure\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv1D(32, kernel_size=10, activation='relu', input_shape=(npArr.shape[1],npArr.shape[2])))\n",
    "model.add(layers.SpatialDropout1D(rate=0.2))\n",
    "#model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Conv1D(64, kernel_size=5, activation='relu'))\n",
    "model.add(layers.SpatialDropout1D(rate=0.4))\n",
    "#model.add(layers.BatchNormalization())\n",
    "\n",
    "#model.add(layers.Conv1D(256, kernel_size=4, activation='relu'))\n",
    "#model.add(layers.SpatialDropout1D(rate=0.4))\n",
    "#model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Conv1D(128, kernel_size=4, activation='relu'))\n",
    "model.add(layers.SpatialDropout1D(rate=0.4))\n",
    "#model.add(layers.BatchNormalization())\n",
    "\n",
    "#model.add(layers.Conv1D(64, kernel_size=4, activation='relu'))\n",
    "#model.add(layers.SpatialDropout1D(rate=0.4))\n",
    "#model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(5000, activation = 'relu'))\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "model.add(layers.Dense(num_classes, activation= 'softmax'))\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", patience=10), ModelCheckpoint(filepath=\"best_cnn_model.h5\", \n",
    "                                                                           monitor=\"val_loss\",save_best_only=True)]\n",
    "\n",
    "adam = optimizers.Adam(lr = 0.001, decay = 0.0001)\n",
    "\n",
    "#compile model using accuracy to measure model performance\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5108 samples, validate on 1277 samples\n",
      "Epoch 1/200\n",
      " 416/5108 [=>............................] - ETA: 3:45 - loss: nan - acc: 0.0409"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-d520c81a4e36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m history = model.fit(npArr, enc_labelArr, epochs=epochs, verbose=1, batch_size=batch_size, \n\u001b[0;32m----> 6\u001b[0;31m                       callbacks=callbacks, validation_split = 0.2)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#specify hyperparameters\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(npArr, enc_labelArr, epochs=epochs, verbose=1, batch_size=batch_size, \n",
    "                      callbacks=callbacks, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maybe 2D Conv Model?\n",
    "maybe 1D Conv Layer then a couple 2D Conv?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.6515067e-02, -6.7485064e-02, -4.0974207e-02, ...,\n",
       "         3.4078300e-07, -2.1590799e-06,  0.0000000e+00],\n",
       "       [ 2.1176578e-02, -2.7541181e-02, -4.2534634e-02, ...,\n",
       "        -6.9564326e-06,  4.9421424e-06,  9.0536887e-06],\n",
       "       [ 5.3427730e-02, -1.5425559e-02, -2.5265059e-02, ...,\n",
       "        -1.6132236e-06,  7.0478754e-06,  2.3184088e-05],\n",
       "       [ 5.1594611e-02, -3.2933217e-02, -8.1078112e-02, ...,\n",
       "        -6.1627206e-06, -1.0733088e-06,  7.5541160e-05],\n",
       "       [ 2.4223570e-02, -3.6726356e-02, -3.2027334e-02, ...,\n",
       "         1.7665525e-06,  7.1124373e-07,  3.2856657e-05]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset\n",
    "feature_set = pd.read_csv(dataFolder + 'new_feature_df.csv')\n",
    "feature_set.replace(np.NaN, 0.0, inplace=True)\n",
    "feature_set.replace(np.inf, 4.0, inplace=True)\n",
    "\n",
    "# get trainDf again if top isn't run fully\n",
    "trainDf = pd.read_csv(dataFolder+trainData)\n",
    "trainDf = trainDf[trainDf.Label != 'lay']\n",
    "\n",
    "#merge labels to get training set\n",
    "feature_set = pd.merge(feature_set, trainDf, left_on='id', right_on = 'Datafile', how = 'inner')\n",
    "fileorder = feature_set['id']\n",
    "\n",
    "# one-hot encode label\n",
    "from keras.utils import to_categorical\n",
    "factor = pd.factorize(feature_set.Label)\n",
    "feature_set.Label = factor[0]\n",
    "labels = to_categorical(feature_set['Label'].values)\n",
    "\n",
    "feature_set.drop(columns= ['id', 'Subject', 'Datafile'], inplace = True)\n",
    "\n",
    "\n",
    "#normalize feature set\n",
    "f = feature_set.values.astype('float32')\n",
    "f = normalize(f, axis = 1)\n",
    "f[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create MLP structure\n",
    "mlp = models.Sequential()\n",
    "mlp.add(layers.Dense(2640, activation='relu', input_dim=264))\n",
    "mlp.add(layers.Dropout(rate=0.2))\n",
    "mlp.add(layers.Dense(1280, activation = 'relu'))\n",
    "mlp.add(layers.Dropout(rate=0.5))\n",
    "mlp.add(layers.Dense(800, activation = 'relu'))\n",
    "mlp.add(layers.Dropout(rate=0.5))\n",
    "mlp.add(layers.Dense(400, activation = 'relu'))\n",
    "mlp.add(layers.Dropout(rate=0.5))\n",
    "mlp.add(layers.Dense(22, activation= 'softmax'))\n",
    "\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", patience=10), ModelCheckpoint(filepath=\"best_mlp_model.h5\", \n",
    "                                                                           monitor=\"val_loss\",save_best_only=True)]\n",
    "\n",
    "adam = optimizers.Adam(lr = 0.001)\n",
    "sgd = optimizers.SGD(lr = 0.001, momentum = 0.1, decay = 0.001)\n",
    "#compile model using accuracy to measure model performance\n",
    "mlp.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5108 samples, validate on 1277 samples\n",
      "Epoch 1/200\n",
      "5108/5108 [==============================] - 14s 3ms/step - loss: 2.5054 - acc: 0.1754 - val_loss: 2.0944 - val_acc: 0.2553\n",
      "Epoch 2/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 1.6522 - acc: 0.3604 - val_loss: 1.8482 - val_acc: 0.3547\n",
      "Epoch 3/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 1.2091 - acc: 0.4955 - val_loss: 1.2940 - val_acc: 0.4894\n",
      "Epoch 4/200\n",
      "5108/5108 [==============================] - 12s 2ms/step - loss: 0.9715 - acc: 0.5781 - val_loss: 1.3999 - val_acc: 0.5129\n",
      "Epoch 5/200\n",
      "5108/5108 [==============================] - 12s 2ms/step - loss: 0.8481 - acc: 0.6341 - val_loss: 1.1263 - val_acc: 0.5873\n",
      "Epoch 6/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.8224 - acc: 0.6443 - val_loss: 1.4963 - val_acc: 0.5411\n",
      "Epoch 7/200\n",
      "5108/5108 [==============================] - 12s 2ms/step - loss: 0.6800 - acc: 0.7034 - val_loss: 1.2380 - val_acc: 0.5442\n",
      "Epoch 8/200\n",
      "5108/5108 [==============================] - 12s 2ms/step - loss: 0.6598 - acc: 0.7120 - val_loss: 1.2010 - val_acc: 0.5818\n",
      "Epoch 9/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.6350 - acc: 0.7306 - val_loss: 1.1419 - val_acc: 0.6006\n",
      "Epoch 10/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.5788 - acc: 0.7500 - val_loss: 1.2274 - val_acc: 0.5834\n",
      "Epoch 11/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.5763 - acc: 0.7580 - val_loss: 1.3360 - val_acc: 0.5779\n",
      "Epoch 12/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.5505 - acc: 0.7657 - val_loss: 1.1067 - val_acc: 0.6406\n",
      "Epoch 13/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.5074 - acc: 0.7884 - val_loss: 1.0319 - val_acc: 0.6547\n",
      "Epoch 14/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.5138 - acc: 0.7864 - val_loss: 1.2575 - val_acc: 0.5873\n",
      "Epoch 15/200\n",
      "5108/5108 [==============================] - 12s 2ms/step - loss: 0.4836 - acc: 0.7962 - val_loss: 1.2071 - val_acc: 0.6030\n",
      "Epoch 16/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.4503 - acc: 0.8125 - val_loss: 1.1345 - val_acc: 0.6359\n",
      "Epoch 17/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.4207 - acc: 0.8332 - val_loss: 1.2094 - val_acc: 0.6202\n",
      "Epoch 18/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.4225 - acc: 0.8356 - val_loss: 1.2694 - val_acc: 0.6116\n",
      "Epoch 19/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.4152 - acc: 0.8285 - val_loss: 1.0938 - val_acc: 0.6617\n",
      "Epoch 20/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.3928 - acc: 0.8457 - val_loss: 1.1286 - val_acc: 0.6460\n",
      "Epoch 21/200\n",
      "5108/5108 [==============================] - 12s 2ms/step - loss: 0.3669 - acc: 0.8549 - val_loss: 1.1180 - val_acc: 0.6421\n",
      "Epoch 22/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.3790 - acc: 0.8532 - val_loss: 1.2205 - val_acc: 0.6351\n",
      "Epoch 23/200\n",
      "5108/5108 [==============================] - 11s 2ms/step - loss: 0.3488 - acc: 0.8610 - val_loss: 1.1110 - val_acc: 0.6609\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "hist = mlp.fit(f, labels, batch_size=batch_size, epochs=epochs, callbacks=callbacks, \n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN plus feature MLP\n",
    "inspired by https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "feature_set = pd.read_csv(dataFolder + 'new_feature_df.csv')\n",
    "feature_set.replace(np.NaN, 0.0, inplace=True)\n",
    "feature_set.replace(np.inf, 4.0, inplace=True)\n",
    "\n",
    "# get trainDf again if top isn't run fully\n",
    "trainDf = pd.read_csv(dataFolder+trainData)\n",
    "trainDf = trainDf[trainDf.Label != 'lay']\n",
    "\n",
    "#merge labels to get training set\n",
    "feature_set = pd.merge(feature_set, trainDf, left_on='id', right_on = 'Datafile', how = 'inner')\n",
    "fileorder = feature_set['id']\n",
    "\n",
    "feature_set.drop(columns= ['id', 'Subject', 'Datafile', 'Label'], inplace = True)\n",
    "\n",
    "#normalize feature set\n",
    "f = feature_set.values\n",
    "f = (f - f.min(axis=1, keepdims=True)) / (f.max(axis=1, keepdims=True) - f.min(axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Network Structure\n",
    "# import the necessary packages\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import concatenate\n",
    "\n",
    "#input_ts = Input(shape=(npArr.shape[1], npArr.shape[2]))\n",
    "#input_f = Input(shape=f_train.shape[1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/BBDC-GTA/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/BBDC-GTA/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "## creating CNN component\n",
    "# initialize the input shape and channel dimension, assuming\n",
    "# TensorFlow/channels-last ordering\n",
    "    \n",
    "inputShape = (npArr.shape[1], npArr.shape[2],)\n",
    "chanDim = -1\n",
    " \n",
    "# define the model input\n",
    "input_ts = Input(shape=inputShape)\n",
    " \n",
    "# CONV => RELU => BN => POOL\n",
    "c = Conv1D(32, 10, strides=10, activation = 'relu')(input_ts)\n",
    "c = BatchNormalization(axis=chanDim, momentum = 0.0)(c)\n",
    "c = MaxPooling1D(pool_size=2)(c)\n",
    "c = SpatialDropout1D(rate=0.2)(c)\n",
    "    \n",
    "c = Conv1D(64, 10, activation = 'relu')(c)\n",
    "c = BatchNormalization(axis=chanDim, momentum = 0.0)(c)\n",
    "c = MaxPooling1D(pool_size=2)(c)\n",
    "c = SpatialDropout1D(rate=0.2)(c)\n",
    "    \n",
    "c = Conv1D(128, 4, activation = 'relu')(c)\n",
    "c = BatchNormalization(axis=chanDim, momentum = 0.0)(c)\n",
    "c = MaxPooling1D(pool_size=2)(c)\n",
    "c = SpatialDropout1D(rate=0.2)(c)\n",
    "        \n",
    "# flatten the volume, then FC => RELU => BN => DROPOUT\n",
    "c = Flatten()(c)\n",
    "c = Dense(2000, activation='relu')(c)\n",
    "c = Dropout(0.4)(c)\n",
    " \n",
    "# apply another FC layer, this one to match the number of nodes\n",
    "# coming out of the MLP\n",
    "c_out = Dense(500, activation='relu')(c)\n",
    "\n",
    "### create simple MLP\n",
    "#model input\n",
    "input_m = Input(shape=(264,))\n",
    "\n",
    "#two dense layers with dropout in between\n",
    "m = Dense(1000, activation=\"relu\")(input_m)\n",
    "m = BatchNormalization(axis=chanDim, momentum = 0.0)(m)\n",
    "m = Dropout(rate=0.2)(m)\n",
    "m = Dense(1000, activation=\"relu\")(input_m)\n",
    "m = Dropout(rate=0.4)(m)\n",
    "m_out = Dense(500, activation=\"relu\")(m)\n",
    "\n",
    "# create the input to our final set of layers as the *output* of both\n",
    "# the MLP and CNN\n",
    "comb = concatenate([c_out, m_out])\n",
    "\n",
    "# our final FC layer head will have two dense layers, the final one\n",
    "# being our regression head\n",
    "comb = Dense(1000, activation=\"relu\")(comb)\n",
    "comb = Dropout(rate = 0.5)(comb)\n",
    "out = Dense(22, activation=\"softmax\")(comb)\n",
    "\n",
    "model = Model(inputs=[input_ts, input_m], outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify optimizers\n",
    "adam = Adam(lr = 0.001, decay = 0.001 / 200)\n",
    "\n",
    "#compile model using accuracy to measure model performance\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/BBDC-GTA/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 5108 samples, validate on 1277 samples\n",
      "Epoch 1/200\n",
      "5108/5108 [==============================] - 65s 13ms/step - loss: 2.5863 - acc: 0.2101 - val_loss: 1.3896 - val_acc: 0.4839\n",
      "Epoch 2/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 1.3617 - acc: 0.5305 - val_loss: 0.9725 - val_acc: 0.6711\n",
      "Epoch 3/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.8792 - acc: 0.6962 - val_loss: 0.7243 - val_acc: 0.7439\n",
      "Epoch 4/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.6472 - acc: 0.7892 - val_loss: 0.6493 - val_acc: 0.7886\n",
      "Epoch 5/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.5158 - acc: 0.8289 - val_loss: 0.8706 - val_acc: 0.7619\n",
      "Epoch 6/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.4650 - acc: 0.8496 - val_loss: 0.8612 - val_acc: 0.7776\n",
      "Epoch 7/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.4056 - acc: 0.8682 - val_loss: 0.6081 - val_acc: 0.8168\n",
      "Epoch 8/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.3610 - acc: 0.8821 - val_loss: 0.7228 - val_acc: 0.8215\n",
      "Epoch 9/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.3470 - acc: 0.8923 - val_loss: 0.7292 - val_acc: 0.8097\n",
      "Epoch 10/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.2976 - acc: 0.8970 - val_loss: 0.7127 - val_acc: 0.8081\n",
      "Epoch 11/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.3255 - acc: 0.8970 - val_loss: 0.8164 - val_acc: 0.7925\n",
      "Epoch 12/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.2768 - acc: 0.9164 - val_loss: 0.8801 - val_acc: 0.7659\n",
      "Epoch 13/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.2803 - acc: 0.9233 - val_loss: 0.8262 - val_acc: 0.8081\n",
      "Epoch 14/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.2288 - acc: 0.9309 - val_loss: 0.7978 - val_acc: 0.8066\n",
      "Epoch 15/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.2290 - acc: 0.9301 - val_loss: 0.7547 - val_acc: 0.8277\n",
      "Epoch 16/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.2031 - acc: 0.9330 - val_loss: 0.9535 - val_acc: 0.7956\n",
      "Epoch 17/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.1898 - acc: 0.9424 - val_loss: 0.7214 - val_acc: 0.8285\n",
      "Epoch 18/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.2060 - acc: 0.9383 - val_loss: 0.9019 - val_acc: 0.8363\n",
      "Epoch 19/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.1505 - acc: 0.9499 - val_loss: 0.7710 - val_acc: 0.8403\n",
      "Epoch 20/200\n",
      "5108/5108 [==============================] - 63s 12ms/step - loss: 0.2082 - acc: 0.9417 - val_loss: 0.7405 - val_acc: 0.8191\n",
      "Epoch 21/200\n",
      "1088/5108 [=====>........................] - ETA: 46s - loss: 0.1818 - acc: 0.9522"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8a30d0b698c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m model.fit([npArr, f], enc_labelArr, batch_size=batch_size, epochs=epochs, callbacks=callbacks, \n\u001b[0;32m---> 10\u001b[0;31m           validation_split=0.2, shuffle = True)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#specify hyperparameters\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "#early stopping\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", patience=30), ModelCheckpoint(filepath=\"best_comb_model.h5\", \n",
    "                                                                           monitor=\"val_loss\",save_best_only=True)]\n",
    "\n",
    "model.fit([npArr, f], enc_labelArr, batch_size=batch_size, epochs=epochs, callbacks=callbacks, \n",
    "          validation_split=0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN plus earlier averaged RF models\n",
    "Source: https://www.mdpi.com/1424-8220/16/1/115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/BBDC-GTA/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/BBDC-GTA/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/BBDC-GTA/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "cnn = load_model('best_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnpred = cnn.predict(testArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8  9  4 18  4 16  3 21 16  1  3  0 16  9 13 16 11 15 14 13 15  3 19 12\n",
      " 21 20 11  2 13  2  0  8 17 19 18 18 15  6  2 17 13 18  9  5 17 21 20 21\n",
      "  1 15]\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(cnnpred, axis = 1)[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1738, 264)\n",
      "(6385, 265)\n"
     ]
    }
   ],
   "source": [
    "#load dataset\n",
    "feature_set = pd.read_csv(dataFolder + 'new_feature_df.csv')\n",
    "feature_set.replace(np.NaN, 0.0, inplace=True)\n",
    "feature_set.replace(np.inf, 4.0, inplace=True)\n",
    "\n",
    "# get trainDf again if top isn't run fully\n",
    "trainDf = pd.read_csv(dataFolder+trainData)\n",
    "trainDf = trainDf[trainDf.Label != 'lay']\n",
    "\n",
    "#merge labels to get training set\n",
    "train_df = pd.merge(feature_set, trainDf, left_on='id', right_on = 'Datafile', how = 'inner')\n",
    "train_df.drop(columns= ['id', 'Subject', 'Datafile'], inplace = True)\n",
    "\n",
    "factor = pd.factorize(train_df['Label'])\n",
    "train_df.Label = factor[0]\n",
    "\n",
    "#initialize dataframe with test observations\n",
    "test_subjects = pd.read_csv(dataFolder + 'challenge.csv')\n",
    "\n",
    "#create test_df by merging with testFile\n",
    "test_df = pd.merge(feature_set, test_subjects, left_on='id', right_on= 'Datafile', how='inner')\n",
    "test_df.drop(columns = ['Label', 'id', 'Subject', 'Datafile'], inplace = True)\n",
    "\n",
    "print(test_df.shape)\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate old RF models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#train test split\n",
    "f_train, f_test, l_train, l_test= train_test_split(train_df.iloc[:, -1:], train_df.iloc[:, -1], test_size = 0.2, random_state=7)\n",
    "\n",
    "# hyperparameters\n",
    "random_state_range = range(40, 60)\n",
    "n_trees = 100\n",
    "\n",
    "#initialize probability array\n",
    "prob = np.empty((len(test_df), 22))\n",
    "\n",
    "#loop to run Random Forest in different random states and add the probabilities obtained from each to prob\n",
    "cnt = 0\n",
    "for random_state in random_state_range:\n",
    "    classifier = RandomForestClassifier(n_estimators = n_trees, random_state = random_state)\n",
    "    classifier.fit(train_df.iloc[:, :-1], train_df.iloc[:, -1])\n",
    "    prob += classifier.predict_proba(test_df)\n",
    "    cnt += 1\n",
    "\n",
    "#argmax function to choose the label with highest cumulative probability\n",
    "cum_pred = np.argmax(prob, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17, 12,  9, ...,  1,  8,  5])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cum_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1738"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(indyArr == test_df['Datafile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.121320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.949747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.670832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.031189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.192388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.222524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11.157957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>12.020815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>12.825755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>13.583078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>14.300350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>14.983324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>15.636496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16.263456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16.867128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>17.449928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>18.013884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>18.560711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>19.091883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>19.608672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>20.112185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>20.603398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>21.083169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>21.552262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>22.011361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>22.461077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>22.901965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>23.334524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>23.759209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>24.176435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0          0\n",
       "0   0.0   2.121320\n",
       "1   0.0   4.949747\n",
       "2   0.0   6.670832\n",
       "3   0.0   8.031189\n",
       "4   0.0   9.192388\n",
       "5   0.0  10.222524\n",
       "6   0.0  11.157957\n",
       "7   0.0  12.020815\n",
       "8   0.0  12.825755\n",
       "9   0.0  13.583078\n",
       "10  0.0  14.300350\n",
       "11  0.0  14.983324\n",
       "12  0.0  15.636496\n",
       "13  0.0  16.263456\n",
       "14  0.0  16.867128\n",
       "15  0.0  17.449928\n",
       "16  0.0  18.013884\n",
       "17  0.0  18.560711\n",
       "18  0.0  19.091883\n",
       "19  0.0  19.608672\n",
       "20  0.0  20.112185\n",
       "21  0.0  20.603398\n",
       "22  0.0  21.083169\n",
       "23  0.0  21.552262\n",
       "24  0.0  22.011361\n",
       "25  0.0  22.461077\n",
       "26  0.0  22.901965\n",
       "27  0.0  23.334524\n",
       "28  0.0  23.759209\n",
       "29  0.0  24.176435"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.DataFrame(np.zeros(300))\n",
    "x = x.rolling(10).mean().dropna(axis='index').iloc[::10].apply(np.sqrt).reset_index(drop = True)\n",
    "\n",
    "y = pd.DataFrame(list(range(600)))\n",
    "y = y.rolling(10).mean().dropna(axis='index').iloc[::20].apply(np.sqrt).reset_index(drop = True)\n",
    "\n",
    "xy = pd.concat([x, y], axis = 1)\n",
    "xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.12132034,  2.12132034],\n",
       "       [ 3.80788655,  4.94974747],\n",
       "       [ 4.94974747,  6.67083203],\n",
       "       [ 5.87367006,  8.0311892 ],\n",
       "       [ 6.67083203,  9.19238816],\n",
       "       [ 7.38241153, 10.22252415],\n",
       "       [ 8.0311892 , 11.1579568 ],\n",
       "       [ 8.63133825, 12.02081528],\n",
       "       [ 9.19238816, 12.82575534],\n",
       "       [ 9.72111105, 13.58307771],\n",
       "       [10.22252415, 14.30034965],\n",
       "       [10.70046728, 14.98332406],\n",
       "       [11.1579568 , 15.63649577],\n",
       "       [11.5974135 , 16.26345597],\n",
       "       [12.02081528, 16.86712779],\n",
       "       [12.42980289, 17.44992837],\n",
       "       [12.82575534, 18.01388353],\n",
       "       [13.20984481, 18.56071119],\n",
       "       [13.58307771, 19.09188309],\n",
       "       [13.94632568, 19.60867155],\n",
       "       [14.30034965, 20.11218536],\n",
       "       [14.64581852, 20.60339778],\n",
       "       [14.98332406, 21.08316864],\n",
       "       [15.31339283, 21.55226206],\n",
       "       [15.63649577, 22.0113607 ],\n",
       "       [15.95305613, 22.46107745],\n",
       "       [16.26345597, 22.90196498],\n",
       "       [16.56804153, 23.33452378],\n",
       "       [16.86712779, 23.75920874],\n",
       "       [17.1610023 , 24.17643481]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = xy.values\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2., -1.,  0.,  1.,  2.],\n",
       "       [-2., -1.,  0.,  1.,  2.],\n",
       "       [-2., -1.,  0.,  1.,  2.],\n",
       "       [-2., -1.,  0.,  1.,  2.]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = array - array.mean(axis = 1, keepdims = True)\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [0, 1, 2, 3, 4]\n",
       "Index: []"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = pd.DataFrame(arr)\n",
    "arr = arr.rolling(2, axis = 0).mean().dropna(axis = 0).iloc[::5, :].reset_index(drop = True)\n",
    "\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4\n",
       "0  4.0  1.0  0.0  1.0  4.0"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.iloc[::3, :].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4\n",
       "1  2.0  1.0  0.0  1.0  2.0\n",
       "2  2.0  1.0  0.0  1.0  2.0\n",
       "3  2.0  1.0  0.0  1.0  2.0"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = arr ** 2\n",
    "arr.apply(np.sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
